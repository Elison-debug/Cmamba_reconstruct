\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage[table]{xcolor}
\usepackage{float}
\usepackage{placeins}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{cite}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\renewcommand{\IEEEkeywordsname}{Keywords}

% Use SVG without LaTeX text overlay to reduce external file requirements.
\setsvg{inkscapelatex=false}

% Keep table text size consistent throughout the paper.
% (Some tables use \scriptsize; map it to \footnotesize to avoid odd-looking tiny text.)
\let\origScriptsize\scriptsize
\renewcommand{\scriptsize}{\footnotesize}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

% Author markup (toggle by redefining environments to empty if needed)
\newenvironment{CZX}{\color{blue}}{}
\newenvironment{SJC}{\color{red}}{}
% \renewenvironment{CZX}{}{}
% \renewenvironment{SJC}{}{}

\begin{document}

\title{Efficient Implementation of Mamba Architecture for Radio-based Localisation Using LuViRA datasest}

\author{Chenxi Zhang and Shengjie Chen}

% The paper headers
\markboth{Report for Advanced Course in Electrical and Information Technology - EITN35}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle
{\footnotesize
Blue text in the Abstract, Introduction, Conclusion, and Section~II is by
Chenxi; all remaining sections are by Shengjie.\par}

\begin{abstract}
\begin{CZX}
This paper investigates three Mamba-based models variant on the LuViRA\cite{yaman2024luvira} 
dataset and demonstrates the effectiveness of SSM-based architectures 
for indoor radio localization. Building on these findings, we propose 
task-oriented simplifications that preserve comparable inference 
accuracy while significantly reducing model size, hardware cost, 
and end-to-end training/inference overhead.
\end{CZX}
To further enable deployment, 
we implement a synthesizable FPGA accelerator for the state-space
model (SSM) hotspot in Mamba, targeting indoor radio localization (LuViRA).
Software profiling identifies the SSM forward path as a dominant runtime
contributor. We implement SSM as a streaming pipeline using three primitives
(MAC, EWA, EWM) and a LUT-based sigmoid, with pipelined
MVM arrays, interleaving memory access, and FIFO-based AXI-Stream control for
backpressure-safe composition. The integrated module closes timing at
400~MHz using 76 DSPs and 6{,}244 LUTs, with an estimated on-chip power of
1.44~W, providing a baseline toward end-to-end Mamba inference. To support reproducibility, the accelerator implementation is open-sourced at \url{https://github.com/Elison-debug/slim_mamba}.
\end{abstract}

\begin{IEEEkeywords}
Mamba, state-space model (SSM), indoor localization, massive MIMO, FPGA
acceleration.
\end{IEEEkeywords}

\section{Introduction}
%Chenxi: modified the first paragraph
\begin{CZX}
\IEEEPARstart{I}{ndoor} radio localization requires modeling 
high-dimensional wireless signals with strong temporal dynamics and 
multipath effects. As antenna/channel dimensions increase, conventional 
FCNN-based regressors become inefficient: their parameter count and compute 
cost grow rapidly with input width, making deployment and scaling difficult\cite{tian2023deep}. 
This motivates architectures that can process long signal sequences
with stronger parameter efficiency.

Mamba, as a modern state-space-model (SSM) architecture, offers a compact alternative
to dense FCNNs by summarizing temporal context through recurrent state updates with
linear-time sequence processing. For indoor localization, this enables efficient use
of temporal dynamics without the parameter explosion of FCNNs.

In this project, we follow a software-to-hardware co-design path on the LuViRA
dataset~\cite{yaman2024luvira} and evaluate three Mamba-family variants for radio 
localization: vanilla Mamba v1\cite{gu2024mamba}, channel mamba\cite{zeng2024cmamba}, and slim mamba. We first adopted a channel mamba design\cite{zeng2024cmamba} to support multi-domain 
feature fusion (e.g., CIR real/imag, power, and optional differential features), 
motivated by potential cross-channel interaction and extensibility to future multi-modal 
inputs (e.g., audio). However, experiments show that, under the current setting, additional 
channel-fusion branches bring only limited accuracy gain while 
introducing substantial overhead in model complexity, training time, and hardware mapping cost.
For clarity, we refer to the channel-fusion variant as \emph{channel mamba} and the simplified model as \emph{slim mamba} throughout the report.
Overall, slim mamba achieves comparable test accuracy to channel mamba and the FCNN baseline while
reducing the model size to 3.40~MB (0.77\% of FCNN) and the training time to 13~s per epoch
(Table~\ref{tab:model_compare}).

\begin{figure}[H]
  \centering
  \includesvg[width=0.75\linewidth]{MAMBAFlow}
  \caption{Algorithmic view of the slim mamba block~\cite{gu2024mamba}.}
  \label{fig:ssm_algo}
\end{figure}

Unlike FCNN baselines that perform single-snapshot localization, Mamba processes a short sequence
of observations jointly, enabling multi-snapshot localization and exploiting temporal dynamics in
the wireless channel. Based on this observation, we first examined the temporal context length within the
channel mamba prototype and found that a short window (\(K=16\)) already achieves strong accuracy.
This indicates that very long-range memory is not necessary for this short-horizon localization task
and may introduce redundancy. We therefore proposed a hardware-friendly simplification of Mamba v1
that preserves comparable inference accuracy while reducing model size and end-to-end runtime overhead.
The final slim mamba architecture is in Fig.~\ref{fig:ssm_algo}.
\end{CZX}

From a deployment perspective, acceleration should target real runtime hotspots.
Profiling (Table~\ref{tab:ssm_profile}) shows that the SSM forward path dominates execution,
so we implement it with a fully synthesizable FPGA design. The accelerator uses a streaming
pipeline built from MAC, EWA, and EWM operators plus a LUT-based sigmoid, and scales via a
$4\times4\times4$ pipelined MVM array, interleaving memory access, and FIFO-based AXI-Stream flow
control for backpressure-safe composition.

Although recent studies have explored Mamba acceleration from complementary angles, 
including hardware--algorithm co-design~\cite{wang2025fastmamba}, reconfigurable 
operator support~\cite{li2024marca}, and hybrid dataflows~\cite{jin2025hcsas}, 
several trade-offs remain under-discussed for radio-localization-oriented FPGA deployment. 
First, compute-array organization is often fixed without controlled comparison under 
identical workload assumptions. Second, memory organization for mixed linear and 
element-wise SSM pipelines is not sufficiently analyzed, especially for interleaved 
banked access. Third, control granularity trade-offs between coarse scheduling and 
fine-grained streaming are rarely quantified under the same constraints.

Motivated by these gaps, this work contributes: 1) an empirical software-side study 
from channel mamba to slim mamba for short-window localization, 2) a 
hotspot-driven FPGA mapping of the SSM datapath, and 3) a consistent evaluation of array, 
memory, and control trade-offs. The rest of this report first presents SSM decomposition 
and hardware mapping, then details the accelerator architecture and implementation results.

% In practical Mamba implementations, the computational bottleneck is frequently
% concentrated in the SSM core operator. Our
% function-level profiling of the current software stack as summarized in
% Table~\ref{tab:ssm_profile} indicates that the SSM-related forward path
% accounts for a substantial fraction of runtime, making SSM the most
% promising target for hardware acceleration. Therefore, the core focus of this
% course project is to design a synthesizable FPGA accelerator for the
% SSM-centric dataflow and produce measurable trade-offs
% across compute-array organization, memory accesses, and streaming
% control. Prior work has advanced Mamba acceleration from complementary angles,
% including hardware--algorithm co-design for FPGA deployment
% \cite{wang2025fastmamba}, reconfigurable architectures to better handle the
% mixed ratio of linear and element-wise operations \cite{li2024marca}, and
% hybrid systolic-style organizations with unified state-space buffers and
% energy-efficient dataflows \cite{jin2025hcsas}. Building on these insights,
% our work puts the SSM hotspot as the primary optimization target under
% the radio-localization workload and interface constraints, and develops a FPGA design.

\begin{table}[H]
\caption{Function-level profiling summary.}
\label{tab:ssm_profile}
\centering
\scriptsize
\begin{tabular}{l|c|c}
\hline
\textbf{Function} & \textbf{Time} & \textbf{Share (\%)} \\
\hline
Linear projection (in/out)         & 30  & 7.75  \\
Normalization (RMSNorm)            & 57  & 14.73 \\
SSM dt projection (MVM + sigmoid)  & 140 & 36.18 \\
SSM selective scan (state update)  & 160 & 41.34 \\
\hline
Total (listed)                     & 387 & 100.00 \\
\hline
\end{tabular}
\end{table}

% Although recent hardware studies have proposed various architectures and
% dataflows for accelerating SSM, three recurring bottlenecks are often
% under-discussed when mapping a design to a radio
% localization application workload under realistic FPGA constraints. 

% First, at the compute-array
% level, many works commit to a single array organization, often systolic-style
% or a generic matrix engine and do not systematically compare, under the same
% workload and interface assumptions, how a deeper fine-grained pipelined array
% differs from a classical systolic array or a minimal single-column MVM
% baseline in terms of utilization, control complexity, and throughput
% stability. 

% Second, at the memory level, SSM pipelines couple MVM with
% state-update and element-wise operators, multiple blocks issue read and write operations on
% on-chip buffers; Especially, our new pipelined arrays require a special interleaving memory access.
% However, the impact of interleaving banking is rarely discussed in prior SSM-designs, 
% leaving a gap in memory organization.

% Third, at the system-control level, Some previous
% designs \cite{li2024marca}, \cite{jin2025hcsas} rely on coarse-grained scheduling or state machines, while
% making the hardware reconfigureable to support multiple operators. Some study\cite{wei2025lightmamba} adopts fine-grained 
% streaming with AXI-Stream + FIFO interfaces, but add more hardware cost. The trade-offs between latency and resource cost 
% under different control granularity is not well studied. 

% Motivated by these gaps, our study places these three factors
% into one consistent evaluation framework centered on the SSM hotspot, and
% quantifies three key trade-offs:
% \begin{itemize}
% \item a 4$\times$4$\times$4 pipelined MVM array versus two practical baselines
% \item interleaved banking with single-port versus dual-port memory
% organizations
% \item an AXI-Stream + FIFO fine-grained pipeline versus coarse-grained control
% \end{itemize}

\IEEEpubidadjcol

\section{Software Exploration and Task-Driven Simplification}
This section presents the software-side exploration that guided our final model 
choice and hardware-oriented redesign. We start from a channel mamba architecture 
for multi-domain feature fusion, then progressively simplify the model based on 
empirical evidence under the LuViRA workload.

% figure : data pipeline from raw CSI to model input features
\begin{figure}[t]
  \centering
  \includesvg[width=0.8\linewidth]{dataProcessPipeline}
  \caption{Data preprocessing pipeline from CSI to model-ready features.}
  \label{fig:data_pipeline}
\end{figure}



\subsection{Task Setting and Data Pipeline}
We target indoor localization on LuViRA, where each recording is a time--frequency--space 
CSI tensor (\(T\!\approx\!4000\) at 100\,Hz, 100 antennas, 100 subcarriers). 
To align software processing with deployment-oriented constraints, we organize 
the dataflow in a hardware-friendly manner with sequential window access and compact 
per-frame storage.

Raw CSI is converted into model-ready features through:
(1) IFFT-based CIR extraction over subcarriers, 
(2) tap truncation, and 
(3) feature construction using CIR real/imaginary components, per-antenna log-power, 
and optional first-order temporal difference(see Fig.~\ref{fig:data_pipeline}). 
For each trajectory/grid file, we store \texttt{base.feats.npy}, \texttt{base.xy.npy}, 
and \texttt{base.ts.npy}, while \texttt{base.json} records role-aware window indices 
(train/eval/test), window length, stride, and feature configuration. 
This layout supports on-demand window generation without offline materialization 
of all training samples.

In our final experiments, we enable \texttt{--preload}, which significantly improves 
training throughput by avoiding repeated data fetch/transfer overhead across iterations. 
At the same time, the pipeline still supports lazy-loading mode for resource-constrained 
platforms, and we verified stable operation on a low-end MX330 GPU setup. 
Although preload increases memory usage, the overhead remains acceptable for our workload 
(GPU memory usage rises from approximately 0.3\,GB to 1.4\,GB), while the training speedup 
is substantial. 
Finally, we compute train-only global normalization statistics (\texttt{stats\_train.npz}, 
mean/std with std-floor) to ensure stable scaling across grids and avoid evaluation/test leakage.




\subsection{Initial Design: Channel mamba for Multi-domain Fusion}
Our initial hypothesis was that channel mamba could better exploit heterogeneous 
input domains (e.g., CIR, power, and differential signals), while also providing 
a natural extension path to future multi-modal inputs such as audio.
Accordingly, we first implemented a multi-branch channel mamba model with 
explicit channel fusion blocks.
On LuViRA, this model achieved strong localization accuracy (test mean err \(=12.9\)~cm) 
with a compact serialized size of 40.4~MB, which is substantially smaller than 
the 440~MB FCNN model\cite{tian2023high} reported in prior work (Table~\ref{tab:model_compare}).

\subsection{Empirical Reassessment}
Despite the above motivation, the extra fusion complexity yields only marginal gains
while increasing parameter count, training cost, and hardware mapping overhead. In
addition, differential features bring limited improvement relative to their added
structural complexity, which weakens their deployment value.
From a deployment perspective, channel mamba also poses practical challenges on FPGA:
it relies on several complex nonlinear operators that complicate quantization, stacks
multiple channel mamba blocks that amplify hardware cost, and inherits GPU-oriented kernels
that are not optimized for FPGA dataflow.
Moreover, the original Mamba baseline shows a pronounced generalization gap (training
improves while test accuracy lags), as reflected by the large eval/test discrepancy in
Table~\ref{tab:model_compare}.
Vanilla Mamba v1 also performs markedly worse than channel mamba on this dataset
(Table~\ref{tab:model_compare}), suggesting that its long-context design is not a good fit
for short-horizon localization.
Within the channel mamba baseline, a short context (\(K=16\)) already achieves strong
performance, so we use it as a practical upper bound for temporal context before
simplifying the model.

% Channel Mamba vs Mamba v1
% EPE/MAE, Params, Train Time, Inference Latency, HW Cost Proxy
% =========================================
% Table 1: Channel vs Mamba vs Simplified
% =========================================
\begin{table*}[!t]
\caption{Comparison of model variants on LuViRA (measured results).}
\label{tab:model_compare}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.12}
\begin{tabular}{l|c c c c c c}
\hline
\textbf{Model} & \textbf{Input Features} & \textbf{Model Size (MB)} & \textbf{Train Time / epoch (s)} & \textbf{it/s} & \textbf{Test mean err (cm)} & \textbf{HW Cost Proxy} \\
\hline
FCNN (prior work)\cite{tian2023high}  & CIR          & 440  & N/A & N/A   & \(\sim 13\)    & High \\
channel mamba      & CIR + Power  & 40.4 & 73  & 13.18 & 12.9   & High \\
Vanilla Mamba v1   & CIR + Power  & 3.38 & 33  & 32.62 & 24.74  & Medium \\
slim mamba         & CIR + Power  & 3.40 & 13  & 75.30 & 13.5   & Low \\
\hline
\end{tabular}
\end{table*}

Qualitatively, Fig.~\ref{fig:grid102_error} visualizes the spatial error distribution for a representative grid, highlighting regions with larger localization bias.

\begin{figure}[t]
  \centering
  \includesvg[width=0.8\linewidth]{grid102_error_heatmap}
  \caption{Error heatmap for grid 102.}
  \label{fig:grid102_error}
\end{figure}


\subsection{Task-driven Simplification of Mamba}
We then revisited vanilla Mamba and observed a clear overfitting pattern: the training 
loss continues to decrease while generalization remains poor. The quantitative gap between 
training and test performance is summarized in Table~\ref{tab:model_compare}. This motivates 
a task-driven simplification that reduces model capacity and regularizes the temporal computation 
while keeping the essential selective recurrence.
Based on these findings, we simplify Mamba v1 into a hardware-friendly form by removing non-essential 
complexity while preserving the core recurrent update behavior required by the task.

\subsection{Final Model and Key Differences}
Compared with channel mamba, the final model keeps the core recurrent update path but adopts
a more regular and deployment-friendly structure (fewer branches, reduced state/update complexity,
and more hardware-compatible operators).
This redesign preserves localization accuracy comparable to channel mamba and the FCNN baseline
reported in Table~\ref{tab:model_compare}, while substantially reducing training and inference
time, hardware cost, and quantization difficulty.
The slim mamba SSM follows an EMA-style input-conditioned recurrence, 
followed by a SiLU gate and a 1x1 output projection. This removes the full \((A,B,C,D)\) parameterization 
and low-rank discretization while retaining the temporal update structure (see Fig.~\ref{fig:ssm_algo}).
The error distribution is summarized as a CDF in Fig.~\ref{fig:cdf_zoom}, showing the proportion of samples below a given error threshold.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{cdf_zoom.png}
  \caption{CDF of localization error.}
  \label{fig:cdf_zoom}
\end{figure}


\subsection{Window Length and Patch-Length Ablation}
After fixing slim mamba, we ablated the temporal window length \(K\) together with patch 
length/stride. Table~\ref{tab:k_ablation} shows that \(K=1\) and \(K=2\) achieve the best test accuracy 
(0.135\,m). We attribute this to the dense sampling in LuViRA: short-term dynamics carry strong cues, 
whereas longer windows add redundant context and can even hurt generalization in slim mamba.
We therefore adopt \(K\le 2\) as the default setting for slim mamba in the remainder of the software study.


% =========================================
% Table 2 (wrapped text): Channel vs Slim mamba
% =========================================
\begin{table*}[!t]
\caption{Module-level differences between channel mamba and slim mamba.}
\label{tab:orig_vs_slim}
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.18}
\begin{tabular}{p{0.16\textwidth}|p{0.27\textwidth}|p{0.27\textwidth}|p{0.18\textwidth}}
\hline
\textbf{Component} & \textbf{Channel mamba} & \textbf{Slim mamba (Ours)} & \textbf{Impact} \\
\hline
Input fusion path 
& Multi-branch, channel-fusion-oriented backbone with extra structural overhead. 
& Single-path backbone with no explicit channel-fusion branches. 
& Lower control complexity and easier hardware scheduling. \\
\hline
State update (SSM) 
& Full selective scan parameterization with higher arithmetic and control complexity. 
& Hardware-friendly selective recurrence with reduced state-update complexity. 
& Faster execution and easier FPGA mapping. \\
\hline
Temporal context usage 
& Same short-context setting but more complex fusion paths. 
& Same short-context setting with simplified recurrence. 
& Better task fit; less redundancy and lower overfitting risk. \\
\hline
Operator pattern 
& Mixed operators across fusion branches with heavier scheduling burden. 
& More regular Conv/element-wise-friendly computation flow. 
& Better pipelining and implementation regularity. \\
\hline
Model size / compute 
& Higher parameter and compute cost. 
& Lower parameter and compute cost. 
& Reduced memory footprint and MAC demand. \\
\hline
Quantization difficulty
& Multiple nonlinear paths and branch interactions complicate calibration. 
& Fewer branches with clearer activation flows. 
& Easier calibration and hardware quantization. \\
\hline

\end{tabular}
\end{table*}

% =========================================
% Table 3: K ablation (window length)
% =========================================
\begin{table}[!t]
\caption{Ablation on window length \(K\) with patch length/stride variants (LuViRA).}
\label{tab:k_ablation}
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.12}
\begin{tabular}{c c c}
\hline
\textbf{K (pl, s)} & \textbf{Eval mean err (m)} & \textbf{Test mean err (m)} \\
\hline
1  (1,1) & 0.10068 & \textbf{0.135}  \\
2  (2,2) & 0.09557 & \textbf{0.135}  \\
4  (2,2) & 0.09814 & 0.139   \\
8  (2,2) & 0.09405 & 0.141   \\
4  (4,4) & 0.09983 & 0.137   \\
6  (4,4) & 0.09839 & 0.138   \\
8  (4,4) & 0.09749 & 0.144   \\
8  (8,4) & 0.09744 & 0.152  \\
12 (8,4) & 0.09286 & 0.156  \\
16 (8,4) & 0.09225 & 0.161  \\
24 (8,4) & 0.09432 & 0.159  \\
32 (8,4) & 0.09328 & 0.166  \\
\hline
\end{tabular}
\end{table}


\section{SSM Decomposition and Hardware Mapping}
From an algorithmic perspective, the SSM core can be decomposed into three major steps:
(1) \emph{dt\_projection}, which derives step-size coefficients $\Delta_t$  from the current input;
(2) \emph{state update}, the time-recursive selective scan (recurrent update) that fuses the current input with the previous state to produce a new state $s_t$;
and (3) \emph{output gate}, which combines the updated state with the gating branch to form the output.
This decomposition is consistent with Fig.~\ref{fig:ssm_algo}, where the input is split into a main 
branch and a gate branch after the in\_proj layer; the main branch is locally modulated and then fed 
into the SSM for state update, 
while the output is produced via element-wise interaction with the gate.

When mapped to hardware, the SSM can be reduced to three key hardware primitives:
\begin{itemize}
  \item \textbf{MAC} (matrix--vector multiply--accumulate) for dt\_proj.
  \begin{equation}
    \lambda_t = \sigma(W_{dt} \Delta u_t + b)
  \end{equation}
  \item \textbf{EWA} (element-wise add) for state update.
  \begin{equation}
    s_t = \lambda_t \odot s_{t-1} + (1 - \lambda_t) \odot u_t
  \end{equation}
  \item \textbf{EWM} (element-wise multiply) for output gating.
  \begin{equation}
    y_t = s_t \odot g_t
  \end{equation}
\end{itemize}
Therefore, our design builds a streaming accelerator using \textbf{MVM (MAC) + EWA + EWM} as the core building blocks, centered on the SSM state-update hotspot.

%\subsection{Top-Level Hardware Architecture Overview}
Fig.~\ref{fig:ssm_arch} illustrates the top-level data path of our accelerator, which can be summarized into four stages.
\textbf{(i) Linear compute and reduction}: the input vector is streamed from \texttt{xt\_buf}, weights are supplied by \texttt{WBUF}, and a 4$\times$4$\times$4 MAC array produces partial sums that are reduced by a reduction tree, forming the linear-layer outputs.
\textbf{(ii) Nonlinearity and gate preparation}: the linear outputs are bias-adjusted, decoupled by a FIFO, and passed through a Sigmoid LUT to generate gate-related coefficients, aligning with the gate path.
\textbf{(iii) State update (Element-wise update + s\_buffer)}: a \texttt{Join} module aligns the required streams (e.g., $\lambda$ and $x_t$) and feeds the element-wise update unit that performs combined \textbf{EWM/EWA} operations to produce $s_{\text{new}}$, with \texttt{s\_buffer} closing the read-old/write-new loop for recurrent state updates.
\textbf{(iv) Output gate}: the updated state and the gate stream are aligned by another \texttt{Join}, decoupled by a FIFO, and multiplied element-wise in the EWM gate to produce $y_{\text{out}}$.

This organization keeps the SSM workload concentrated on the \textbf{MAC array (MVM)} and \textbf{element-wise (EWA/EWM)} modules, while FIFO and AXI-streaming boundaries enable fine-grained pipelining and robust backpressure propagation.

% --- figure placeholders ---
\begin{figure*}[t]
  \centering
  \includesvg[width=0.8\linewidth]{ssm_archi}
  \caption{Top-level architecture of the proposed accelerator (based on the Mamba/SSM operator structure)~\cite{gu2024mamba}.}
  \label{fig:ssm_arch}
\end{figure*}

\section{Method}
\subsection{MAC Array Design and Architecture Selection}
This chapter presents the hardware implementation methods of our SSM
accelerator, starting from the compute backbone: the MAC array. For the SSM
datapath, linear layers dominate the arithmetic
cost and can be reduced to an MVM of size
$(256\times256)\cdot(256\times1)$, i.e., $256\times256=65{,}536$ MAC
operations. A useful throughput bound is
\begin{equation}
CC_{\min}=\left\lceil\frac{\text{Total MAC operations}}{\#\text{PEs}}\right\rceil,
\end{equation}
which makes explicit the fundamental trade-off between parallelism (DSP/PE
count) and latency in cycles.

We compare three candidate array organizations.
\begin{enumerate}
  \item A 16$\times$16 systolic array offers strong spatial reuse for GEMM 
  (both operands reuse across the 2D array) and can theoretically complete a tile 
  in 256 cycles. For GEMV, weights have little spatial reuse (each $W_{i,k}$ is 
  typically consumed once per input), making the design weight-bandwidth dominated: 
  full utilization would require streaming $\sim$256 weights/cycle, which also increases 
  routing pressure and complicates timing closure in practice.
  \item A 64$\times$1 single-column GEMV engine directly addresses the
  bandwidth bottleneck and is well matched to GEMV-style computation,
  achieving a 1024-cycle latency under a 64 weights/cycle budget. However,
  a full Mamba pipeline also includes matrix--matrix operations beyond the
  SSM core (e.g., input and output linear projection), so a one-column GEMV structure
   is less reusable for those kernels
  and is therefore suboptimal for long-term extensibility.
  
  \begin{table*}[!t]
\caption{Weight input column scheduling for the 4$\times$4$\times$4 pipelined MAC array.}
\label{tab:weight_sched}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|l|l|l|l|l}
\hline
\textbf{Cycle} & \textbf{Array1 Columns} & \textbf{Array2 Columns} & \textbf{Array3 Columns} & \textbf{Array4 Columns} & \textbf{Description} \\
\hline
1  & col0--3   & -- & -- & -- & ARRAY1 preloads first 4$\times$4 block (tile1). \\
2  & col16--19 & col4--7 & -- & -- & ARRAY2 begins tile1. \\
3  & col32--35 & col20--23 & col8--11 & -- & ARRAY3 begins tile1 (3-cycle stagger). \\
4  & col48--51 & col36--39 & col24--27 & col12--15 & ARRAY4 joins; pipeline full. \\
5--16 & continue +16 stride & same & same & same & Steady-state loading of tile1. \\
17 & {\color{blue}\textbf{col256--259 $\rightarrow$ tile2}} & col244--247 & col232--235 & col220--223 & \textbf{ARRAY1 starts tile2 (row4--7).} \\
18 & col272--275 & {\color{blue}\textbf{col260--263 $\rightarrow$ tile2}} & col248--251 & col236--239 & \textbf{ARRAY2 switches to tile2.} \\
19 & col288--291 & col276--279 & {\color{blue}\textbf{col264--267 $\rightarrow$ tile2}} & col252--255 & \textbf{ARRAY3 switches to tile2.} \\
20 & col304--307 & col292--295 & col280--283 & {\color{blue}\textbf{col268--271 $\rightarrow$ tile2}} & \textbf{ARRAY4 switches; tile1 finishes.} \\
21--37 & continue +16 stride & same & same & same & Steady-state operation for tile2. \\
38 & {\color{red}\textbf{tile3 preload}} & -- & -- & -- & ARRAY1 starts tile3. \\
39 & -- & {\color{red}\textbf{tile3 preload}} & -- & -- & ARRAY2 starts tile3. \\
40 & -- & -- & {\color{red}\textbf{tile3 preload}} & -- & ARRAY3 starts tile3. \\
41 & -- & -- & -- & {\color{red}\textbf{tile3 preload}} & ARRAY4 starts tile3 (3-cycle stagger). \\
42--58 & continue +16 stride & same & same & same & Steady tile3 operation. \\
\hline
\end{tabular}}
\end{table*}

  \item Our proposed 4$\times$4$\times$4 pipelined MAC array is a compromise:
  it preserves the 64 weights/cycle bandwidth target and achieves a similar
  1024-cycle-class latency as the 64$\times$1 baseline, while improving
  routing scalability relative to a monolithic systolic array and enabling
  higher reuse across multiple kernels.
\end{enumerate}

The compute engine instantiates four parallel $4\times4$ MAC sub-arrays
(ARRAY1--ARRAY4), forming a 4$\times$4$\times$4 pipeline organization. Each $4\times4$ sub-array consumes a 4-element slice of the input vector $x_t$ together with a $4\times4$ weight block per cycle.
Each array performs a local $4\times4$ MVM and accumulates the partial sums from the last cycle. After 4 cycles, 
all four sub-arrays have be used and the pipeline is fully filled.
Importantly, the sub-array does \textbf{not} directly output four independent column-wise partial sums.
Instead, it maintains accumulation across the tile and produces an \textbf{already-accumulated $4\times4$ partial-result matrix} at its output interface.
This $4\times4$ partial-result matrix is then forwarded to a dedicated \textbf{three-level reduction tree}, which hierarchically reduces the $4\times4$ block into the required vector-form output used by subsequent stages.

With this organization, the MAC fabric focuses on dense local accumulation, while the reduction tree provides a structured and timing-friendly path to obtain the final MVM vector output. After a short fill phase, all four sub-arrays run concurrently in
steady state, hiding control overhead while keeping the compute fabric small
(64 MACs total), which is favorable for FPGA timing closure. 
Table~\ref{tab:weight_sched} provides the full weight-column schedule;
\vspace{1em}

\begingroup
\setlength{\intextsep}{2pt}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\begin{table}[H]
\caption{Column start progression per sub-array.}
\label{tab:col_progress}
\centering
\scriptsize
\setlength{\tabcolsep}{5pt}
\begin{tabular}{c|l|c}
\hline
\textbf{Array} & \textbf{Column Start Points} & \textbf{$\Delta$} \\
\hline
Array1 & 0 $\rightarrow$ 16 $\rightarrow$ 32 $\rightarrow$ 48 $\rightarrow$ 64 & +16 \\
Array2 & 4 $\rightarrow$ 20 $\rightarrow$ 36 $\rightarrow$ 52 & +16 \\
Array3 & 8 $\rightarrow$ 24 $\rightarrow$ 40 & +16 \\
Array4 & 12 $\rightarrow$ 28 & +16 \\
\hline
\end{tabular}
\end{table}
\endgroup

\begin{table}[H]
\caption{Constant 12-column spacing between adjacent arrays within the same cycle.}
\label{tab:col_spacing}
\centering
\scriptsize
\setlength{\tabcolsep}{6pt}
\begin{tabular}{c|c|c|c}
\hline
\textbf{Cycle} & \textbf{A1$\rightarrow$A2 $\Delta$} & \textbf{A2$\rightarrow$A3 $\Delta$} & \textbf{A3$\rightarrow$A4 $\Delta$} \\
\hline
2 & 16--4 = \textbf{12} & -- & -- \\
3 & 32--20 = \textbf{12} & 20--8 = \textbf{12} & -- \\
4 & 48--36 = \textbf{12} & 36--24 = \textbf{12} & 24--12 = \textbf{12} \\
5 & 64--52 = \textbf{12} & 52--40 = \textbf{12} & 40--28 = \textbf{12} \\
\hline
\end{tabular}
\end{table}

Weights are scheduled in 4-column blocks. For each sub-array, the column
start index advances by a fixed stride of +16 every cycle. Within the same
cycle, the four sub-arrays access distinct column blocks with fixed offsets
$\{0,4,8,12\}$, resulting in a constant 12-column spacing between adjacent
arrays.
Table~\ref{tab:col_progress} summarizes the per-array progression; and
Table~\ref{tab:col_spacing} validates the constant 12-column spacing.

\subsubsection*{$x_t$ input scheduling and wavefront tile switch}
The input vector $x_t$ is streamed from \texttt{xt\_buf} in 4-element blocks.
During pipeline fill (cycles 1--4), ARRAY1 starts first and ARRAY2/3/4 join
sequentially; in steady state, all arrays consume the same $x_t$ block for
the active tile. When moving to the next tile, the $x_t$ block update is
aligned with the weight schedule and propagates as a wavefront across the
arrays, minimizing global bubbles. Table~\ref{tab:xt_sched} details the $x_t$
schedule and matches the tile-switch cycles in Table~\ref{tab:weight_sched}.

\begin{table*}[!t]
\caption{$x_t$ input scheduling aligned with the wavefront tile switch across sub-arrays.}
\label{tab:xt_sched}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}p{0.9cm}|>{\centering\arraybackslash}p{2.5cm}|>{\centering\arraybackslash}p{2.5cm}|>{\centering\arraybackslash}p{2.5cm}|>{\centering\arraybackslash}p{2.5cm}|>{\raggedright\arraybackslash}X}
\hline
\textbf{Cycle} & \textbf{Array1} & \textbf{Array2} & \textbf{Array3} & \textbf{Array4} & \textbf{Description} \\
\hline
1  & xt[0:3] & -- & -- & -- & ARRAY1 begins tile1 (xt block0). \\
2  & xt[0:3] & xt[0:3] & -- & -- & ARRAY2 joins tile1. \\
3  & xt[0:3] & xt[0:3] & xt[0:3] & -- & ARRAY3 joins tile1. \\
4  & xt[0:3] & xt[0:3] & xt[0:3] & xt[0:3] & ARRAY4 joins tile1; steady begins. \\
5--16 & xt[0:3] & xt[0:3] & xt[0:3] & xt[0:3] & tile1 steady-state. \\
17 & \textcolor{blue}{\textbf{xt[4:7]}} & xt[0:3] & xt[0:3] & xt[0:3] & \textbf{ARRAY1 starts tile2 (next row-block).} \\
18 & xt[4:7] & \textcolor{blue}{\textbf{xt[4:7]}} & xt[0:3] & xt[0:3] & \textbf{ARRAY2 switches to tile2.} \\
19 & xt[4:7] & xt[4:7] & \textcolor{blue}{\textbf{xt[4:7]}} & xt[0:3] & \textbf{ARRAY3 switches to tile2.} \\
20 & xt[4:7] & xt[4:7] & xt[4:7] & \textcolor{blue}{\textbf{xt[4:7]}} & \textbf{ARRAY4 switches; tile1 finishes.} \\
21--33 & xt[4:7] & xt[4:7] & xt[4:7] & xt[4:7] & tile2 steady-state. \\
34 & \textcolor{red}{\textbf{xt[8:11]}} & xt[4:7] & xt[4:7] & xt[4:7] & ARRAY1 starts tile3. \\
35 & xt[8:11] & \textcolor{red}{\textbf{xt[8:11]}} & xt[4:7] & xt[4:7] & ARRAY2 switches. \\
36 & xt[8:11] & xt[8:11] & \textcolor{red}{\textbf{xt[8:11]}} & xt[4:7] & ARRAY3 switches. \\
37 & xt[8:11] & xt[8:11] & xt[8:11] & \textcolor{red}{\textbf{xt[8:11]}} & ARRAY4 switches; tile2 ends. \\
\hline
\end{tabularx}
\end{table*}

Overall, the 4$\times$4$\times$4 pipelined array provides a favorable balance
among DSP usage, bandwidth demand, latency, and reuse. However, the compromise
shifts complexity to the memory subsystem. The next subsection therefore focuses on the
memory-access design and compares single-port versus dual-port organizations
in terms of power and resource cost.

\subsection{Interleaved Memory Bank Design}
Sustaining the 4$\times$4$\times$4 pipeline MVM array in steady state requires
the weight buffer (WBUF) to deliver stall-free parallel bandwidth, where four
sub-arrays fetch one $4\times4$ column block per cycle. This section formalizes two
conflict types and derives single-port and dual-port realizations under a
unified modulo-unrolling mapping \cite{barua1998memory}.

\subsubsection*{Conflict model}
Space-conflict captures same-cycle port contention when multiple arrays access
the same bank. A single-port bank supports at most one read per cycle, hence
all four arrays must hit distinct banks per cycle. A dual-port bank supports
up to two reads per cycle, allowing two arrays to share one bank.

Temporal-conflict captures cross-cycle reuse hazards under a finite read
latency $L$. A sufficient safety condition is
\begin{equation}
\text{bank reuse distance} \ge L,
\end{equation}
which prevents issuing a new access to a bank before prior reads have been
safely resolved in the pipeline.

\subsubsection*{Modulo-unrolling mapping derived from the array dataflow}
Weights are accessed in 4-column blocks, each corresponding to a $4\times4$
weight block. Define
\begin{equation}
block\_id=\left\lfloor \frac{col}{4}\right\rfloor,\quad n\in\{0,1,2,3\}.
\end{equation}
The schedule enforces a 12-column spacing between adjacent arrays within the
same cycle, yielding a block-domain offset
\begin{equation}
block\_offset=\frac{12}{4}=3.
\end{equation}
Therefore, the four arrays request
\begin{equation}
block\_id,\; block\_id+3,\; block\_id+6,\; block\_id+9
\end{equation}
\begingroup
within a cycle. We adopt the modulo-unrolling mapping
\begin{equation}
\boxed{\;bank\_id=(block\_id+3n)\bmod N_{\text{bank}}\;}
\end{equation}
which provides periodic, hardware-friendly addressing while preserving the
fixed inter-array spacing property for conflict analysis.

\subsubsection*{Single-port design by increasing the bank count}
For single-port banks, eliminating space-conflict requires pairwise distinct
bank IDs in the same cycle, i.e.,
\begin{equation}
3(n_1-n_2)\not\equiv 0\pmod{N_{\text{bank}}},\quad \forall n_1\neq n_2.
\end{equation}
With four arrays and $block\_offset=3$, selecting $N_{\text{bank}}=12$
satisfies this constraint robustly and yields a regular round-robin placement.
Each bank stores every 12th 4-column block in a round-robin interleaving, and
The runtime timeline is provided in Appendix~\ref{app:bank_timelines}
(Table~\ref{tab:bank_timeline_12}), demonstrating that after warm-up four
distinct banks are activated per cycle.
\subsubsection*{Dual-port design by exploiting two reads per bank per cycle}
Dual-port banks relax the same-cycle constraint to ``no more than two reads per
bank per cycle,'' while still requiring temporal safety against the effective
latency $L$. With $N_{\text{bank}}=6$, the per-cycle accesses collapse into two
active banks per cycle, each serving exactly two reads through deterministic
array pairing. Blocks are interleaved round-robin across six banks, and
Table~\ref{tab:bank_timeline_6} in Appendix~\ref{app:bank_timelines} shows the
bank-pair rotation $\{0,3\}\rightarrow\{1,4\}\rightarrow\{2,5\}$. The reuse
distance in this rotation is two cycles; therefore, the schedule is temporally
safe when $L\le 2$.

\endgroup

\subsubsection*{BRAM utilization and power implications}
Table \ref{tab:sp_dp_resource_power} shows the single-port design uses 49 BRAM tiles at top level (48 in WBUF), while the dual-port design uses 46 (45 in WBUF) due to reduced bank replication. Power also favors dual-port: single-port consumes 1.60 W total (0.97  W dynamic, 0.62 W static) versus 1.31 W (0.68 W dynamic, 0.62 W static), consistent with lower memory/interconnect switching. Therefore, the dual-port WBUF offers a better BRAMâ€“power trade-off without degrading throughput and is selected as the default..

% \begin{figure*}[!t]
% \centering
% \subfloat[Single-port power breakdown]{\includegraphics[width=0.48\textwidth]{sp_power}}
% \hfill
% \subfloat[Dual-port power breakdown]{\includegraphics[width=0.48\textwidth]{dp_power}}
% \captionsetup{justification=centering,singlelinecheck=false}
% \caption{Power breakdown comparison between single-port and dual-port WBUF organizations~\cite{wei2025lightmamba}.}
% \label{fig:power_sp_dp}
% \end{figure*}

\begin{table}[t]
\caption{Post-synthesis resource and power comparison between single-port and dual-port WBUF organizations.}
\label{tab:sp_dp_resource_power}
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c}
\hline
\textbf{Metric} & \textbf{Single-port} & \textbf{Dual-port} \\
\hline
Bank count & 12 & 6 \\
Top-level BRAM tiles & 49 & 46 \\
WBUF BRAM tiles & 48 & 45 \\
DSP blocks & 64 & 64 \\
Dynamic power (W) & 0.971 & 0.682 \\
Static power (W) & 0.626 & 0.624 \\
Total on-chip power (W) & 1.597 & 1.306 \\
\hline
\end{tabular}
\end{table}

\subsection{Nonlinear Layer Implementation}
To realize the sigmoid nonlinearity $\sigma(x)=1/(1+e^{-x})$ with low FPGA
overhead, we implement a lookup-table (LUT) approximation. The module consumes
signed Q8.8 inputs (16-bit) and produces unsigned Q0.16 outputs (16-bit).
Inputs are first clamped to $[-4, +4)$. Importantly, this interval is
selected based on empirical workload statistics: sampled $dt\_proj$ values
before the sigmoid are concentrated within this range, which avoids
over-provisioning the LUT in saturation regions while keeping the table size
compact.

After clamping, the address is generated by a direct fixed-point shift. Since
$[-4,+4)$ corresponds to $[-1024,1023]$ in Q8.8, we compute
\begin{equation}
addr = x_{\text{clamp}} + 1024,
\end{equation}
to obtain an index in $[0,2047]$, enabling a 2048-entry LUT that uniformly
covers the effective domain. Fig.~\ref{fig:sigmoid} compares the 
floating-point sigmoid against the hardware LUT outputs (converted from Q0.16), 
showing close agreement at the sampled points, thereby validating the chosen 
clamp range and table resolution.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{sigmoid.png}
  \caption{Sigmoid approximation accuracy: floating-point sigmoid vs.\ HW LUT outputs.}
  \label{fig:sigmoid}
\end{figure}

\subsection{SSM Control and Fine-grained Pipelining}
To reduce inter-module coupling and improve scalability, we adopt an
AXI-stream-like ready/valid interface and insert lightweight FIFOs between
adjacent operators, following the modular streaming philosophy of
LightMamba~\cite{wei2025lightmamba}. Under this semantics, a FIFO
acts as a standardized boundary for rate matching and timing decoupling. Crucially,
the unified interface also simplifies system evolution: extending from an SSM
subgraph toward a full Mamba pipeline becomes a matter of composing compliant
modules with similar AXI-Stream interfaces, which improves project maintainability and integration robustness.

Within this control paradigm, we examine the trade-off between reconfigurable
and fine-grained pipelines. Many SSM accelerators favor reconfigurable
datapaths to reduce resource usage \cite{wang2025fastmamba,li2024marca,jin2025hcsas},
but increases end-to-end waiting time. In contrast, LightMamba advocates fine-grained streaming, where
each MAC tile is immediately forwarded to the sigmoid and element-wise update
(EWM/EWA) stages. In our throughput analysis with $N=64$ tiles, the fine-grained schedule reduces
latency from $T_A=29N$ to $T_B=22N+7$, i.e., from 1856 to 1415 cycles (23.8\%
reduction) while adds only about 12 DSPs per SSM compute path, making the latency gain well
justified. Therefore, this project ultimately selects the fine-grained,
FIFO-based AXI-stream control scheme.

\begin{figure}[t]
  \centering
  \setlength{\textfloatsep}{6pt}
  \includegraphics[width=\linewidth]{fine.png}
  \caption{Fine-grained streaming schedule from LightMamba~\cite{wei2025lightmamba}.}
  \label{fig:fine_stream}
\end{figure}

\section{Conclusion}
\begin{CZX}
From the software perspective, we systematically evaluated Mamba-family variants on LuViRA and
identified that short temporal contexts are sufficient for accurate localization. The channel mamba
baseline delivers strong accuracy while significantly reducing the model size compared to the prior
FCNN, but its multi-branch structure and nonlinear operators complicate quantization and deployment.
Motivated by the observed generalization gap and hardware constraints, we proposed slim mamba, which
preserves comparable accuracy to channel mamba and the FCNN baseline while substantially reducing
training/inference cost and improving deployability. In particular, slim mamba reduces model size to
3.40~MB, which is about 0.77\% of the FCNN size (440~MB), and completes training in 13~s per epoch
(Table~\ref{tab:model_compare}). We further ablate window length and patch settings and show that a
very short window (\(K=1/2\)) already yields the best test accuracy, indicating that dense short-term
dynamics dominate this task (Table~\ref{tab:k_ablation}).
\end{CZX}

From a hardware perspective, this work targets the SSM state-update hotspot in Mamba and presents a fully synthesizable FPGA accelerator with an SSM-centric
streaming datapath. At 400\,MHz, the design closes timing with 76 DSPs and 6.2k LUTs (3.0\% and
2.3\% of the device) and uses 52 BRAM tiles (5.7\%) for fully on-chip buffering of streaming MVM and
recurrent state updates. Post-synthesis power is estimated at 1.44\,W; dynamic power is currently
I/O-dominated because the deliverable implements the SSM module without a full-system wrapper.
Future work will integrate the module into an end-to-end Mamba pipeline to keep data on-chip and
re-estimate power with realistic switching activity.


\begin{table}[t]
\caption{Post-synthesis utilization and power summary for the integrated SSM module at 400\,MHz.}
\label{tab:ssm_post_synth}
\centering
\scriptsize
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l|c}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
LUTs & 6{,}244 (2.28\%) \\
FFs & 4{,}928 (0.90\%) \\
BRAM tiles & 52 (5.70\%) \\
DSP blocks & 76 (3.02\%) \\
Bonded I/Os & 136 (41.46\%) \\
Total on-chip power & 1.442\,W \\
Dynamic power & 0.817\,W \\
Static power & 0.625\,W \\
I/O power (dynamic) & 0.443\,W (54\%) \\
BRAM power (dynamic) & 0.089\,W \\
Signals power (dynamic) & 0.081\,W \\
Clocks power (dynamic) & 0.078\,W \\
Logic power (dynamic) & 0.073\,W \\
DSP power (dynamic) & 0.053\,W \\
\hline
\end{tabular}
\end{table}

Overall, the results show that an SSM-centric accelerator can achieve high
frequency on FPGA with modest resources. Key contributions include: (i) a
primitive-based mapping (MAC/EWA/EWM) that simplifies control; (ii) a pipelined
MVM backbone with structured reduction; (iii) an interleaving memory access method; and (iv) LUT-based sigmoid plus FIFO/ready--valid streaming for robust
composition and backpressure handling.
\section{Reflections on This Course}
This course trained us in practical research methodology: surveying and
critiquing related work, forming testable hypotheses, and iterating on design
choices through evidence and discussion. Starting from a baseline plan to
implement the Mamba SSM hardware module, we went beyond the initial target by
completing the SSM design and analyzing MVM computation, memory-access
strategies, and system-architecture trade-offs. The resulting solution is
tailored to indoor localization and documented in a public GitHub repository
and this report for reproducibility. Participating in the IES Group Meeting
also provided firsthand experience of how research is conducted and
communicated within the department.
% \section{ References}
\appendices
\section{Bank Access Timelines}
\label{app:bank_timelines}

\begin{table}[!t]
\caption{Timeline of bank accesses for $N_{\text{bank}}=12$ (single-port).}
\label{tab:bank_timeline_12}
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c|c|c|c|c|l}
\hline
\textbf{Cycle} & \textbf{A1$\rightarrow$bank} & \textbf{A2$\rightarrow$bank} & \textbf{A3$\rightarrow$bank} & \textbf{A4$\rightarrow$bank} & \textbf{Banks Active} \\
\hline
1  & bank0 & --     & --     & --     & \{0\} \\
2  & bank4 & bank1  & --     & --     & \{4,1\} \\
3  & bank8 & bank5  & bank2  & --     & \{8,5,2\} \\
4  & bank0 & bank9  & bank6  & bank3  & \{0,9,6,3\} \\
5  & bank4 & bank10 & bank7  & bank1  & \{4,10,7,1\} \\
6  & bank8 & bank11 & bank2  & bank5  & \{8,11,2,5\} \\
7  & bank0 & bank3  & bank6  & bank9  & \{0,3,6,9\} \\
8  & bank4 & bank7  & bank10 & bank1  & \{4,7,10,1\} \\
9  & bank8 & bank11 & bank2  & bank5  & \{8,11,2,5\} \\
10 & bank0 & bank3  & bank6  & bank9  & \{0,3,6,9\} \\
11 & bank4 & bank7  & bank10 & bank1  & \{4,7,10,1\} \\
12 & bank8 & bank11 & bank2  & bank5  & \{8,11,2,5\} \\
\hline
\end{tabular}
\end{table}

\begin{table}[!t]
\caption{Timeline of bank accesses for $N_{\text{bank}}=6$ (dual-port).}
\label{tab:bank_timeline_6}
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c|c|c|c|c|l}
\hline
\textbf{Cycle} & \textbf{A1$\rightarrow$bank} & \textbf{A2$\rightarrow$bank} & \textbf{A3$\rightarrow$bank} & \textbf{A4$\rightarrow$bank} & \textbf{Banks Active} \\
\hline
1  & bank0 & --    & --    & --    & \{0\} \\
2  & bank4 & bank1 & --    & --    & \{1,4\} \\
3  & bank2 & bank5 & bank2 & --    & \{2,5\} \\
4  & bank0 & bank3 & bank0 & bank3 & \{0,3\} \\
5  & bank4 & bank1 & bank4 & bank1 & \{1,4\} \\
6  & bank2 & bank5 & bank2 & bank5 & \{2,5\} \\
7  & bank0 & bank3 & bank0 & bank3 & \{0,3\} \\
8  & bank4 & bank1 & bank4 & bank1 & \{1,4\} \\
9  & bank2 & bank5 & bank2 & bank5 & \{2,5\} \\
10 & bank0 & bank3 & bank0 & bank3 & \{0,3\} \\
11 & bank4 & bank1 & bank4 & bank1 & \{1,4\} \\
12 & bank2 & bank5 & bank2 & bank5 & \{2,5\} \\
\hline
\end{tabular}
\end{table}

\bibliographystyle{IEEEtran}
\bibliography{references} % references.bib file 

\end{document}
